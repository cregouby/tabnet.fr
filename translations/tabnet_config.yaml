title:
  original: Configuration for TabNet models
  translation: ~
arguments:
  batch_size:
    original: |-
      (int) Number of examples per batch, large batch sizes are
      recommended. (default: 1024^2)
    translation: |-
      (entier) Nombre d'exemple par batch, le batch doit être de 
      de grande taille (1024^2 par defaut)
  penalty:
    original: |-
      This is the extra sparsity loss coefficient as proposed
      in the original paper. The bigger this coefficient is, the sparser your model
      will be in terms of feature selection. Depending on the difficulty of your
      problem, reducing this value could help (default 1e-3).
    translation: |-
      Le coefficient de frugalité de la fonction de coût, tel que proposé dans le 
      papier de recherche. Plus il est important, plus le modèle sera frugal en 
      selection de covariable. Suivant la difficulté du problème, il est possible
      de réduire la valeur (1e-3 par défaut)
  clip_value:
    original: |-
      If a float is given this will clip the gradient at
      clip_value. Pass \code{NULL} to not clip.
    translation: |-
      Valeur d'écrêtage du gradient si réel, pas d'écrêtage lorsque la valeur est 
      \code{NULL} 
  loss:
    original: |-
      (character or function) Loss function for training (default to mse
      for regression and cross entropy for classification)
    translation: |-
      (chaîne ou fonction) La fonction de coût pour la phase d'entraînement.
      mse par défaut pour la régression, cross-entropy par defaut pour la 
      classification
  epochs:
    original: (int) Number of training epochs.
    translation: (entier) Nonbre d'époques d'entraînement
  drop_last:
    original: |-
      (logical) Whether to drop last batch if not complete during
      training
    translation: |-
      (logique) Suppression du dernier batch s'il est incomplêt lors de la phase
      d'entraînement
  decision_width:
    original: |-
      (int) Width of the decision prediction layer. Bigger values gives
      more capacity to the model with the risk of overfitting. Values typically
      range from 8 to 64.
    translation: |-
      (entier) largeur de la couche de prediction de la décision. Une valeur
      élevée donne de la capacité au modèle et un risque de sur apprentissage.
      Les valeurs usuelles sont entre 8 et 64.
  attention_width:
    original: |-
      (int) Width of the attention embedding for each mask. According to
      the paper n_d = n_a is usually a good choice. (default=8)
    translation: |-
      (entier) largeur de la tête d'attention des embedding pour chaque masque.
      Le papier recommande n_d = n_a comme un choix raisonnable. (8 par défaut)
  num_steps:
    original: |-
      (int) Number of steps in the architecture
      (usually between 3 and 10)
    translation: |-
      (entier) Nombre d'étapes dans l'architecture du réseau. (usuellement
      entre 3 et 10)
  feature_reusage:
    original: |-
      (float) This is the coefficient for feature reusage in the masks.
      A value close to 1 will make mask selection least correlated between layers.
      Values range from 1.0 to 2.0.
    translation: |-
      (réel) Coéfficient de réutilisation des covariables dans le masque. Une 
      valeur proche de 1 diminue la corrélation de la selection de masque entre
      couches. Valuers entre 1.0 et 2.0
  mask_type:
    original: |-
      (character) Final layer of feature selector in the attentive_transformer
      block, either \code{"sparsemax"} or \code{"entmax"}.Defaults to \code{"sparsemax"}.
    translation: |-
      (chaîne) Couche terminale de sélection de variables dans le bloc 
      attentive_transformer. Soit \code{"sparsemax"}, soit \code{"entmax"}. 
      \code{"sparsemax"} par défaut.
  virtual_batch_size:
    original: |-
      (int) Size of the mini batches used for
      "Ghost Batch Normalization" (default=256^2)
    translation: |-
      (entier) Taille du mini-batch utilisé pour le module "Ghost Batch Normalisation".
      256^2 par défaut
  valid_split:
    original: |-
      (\verb{[0, 1)}) The fraction of the dataset used for validation.
      (default = 0 means no split)
    translation: |-
      (\verb{[0, 1)}) Fraction du dataset dédié à la validation.
  learn_rate:
    original: initial learning rate for the optimizer.
    translation: valeur du taux d'apprentissage initial pour l'optimiseur.
  optimizer:
    original: |-
      the optimization method. currently only 'adam' is supported,
      you can also pass any torch optimizer function.
    translation: |-
      Methode d'optimisation. 'adam' est le seul optimizeur supporté, mais peut
      être remplacé par une fonction d'optimisation torch.
  lr_scheduler:
    original: |-
      if \code{NULL}, no learning rate decay is used. If "step"
      decays the learning rate by \code{lr_decay} every \code{step_size} epochs. If "reduce_on_plateau"
      decays the learning rate by \code{lr_decay} when no improvement after \code{step_size} epochs.
      It can also be a \link[torch:lr_scheduler]{torch::lr_scheduler} function that only takes the optimizer
      as parameter. The \code{step} method is called once per epoch.
    translation: |-
      Le taux d'apprentissage est constant lorque \code{NULL}. Lorsque la valeur est "step",
      décremente le taux d'apprentissage par \code{lr_decay} toute les \code{step_size} époques. 
      Le même résultat peut être obtenu avec une fonction \link[torch:lr_scheduler]{torch::lr_scheduler} qui prend
      l'optimiseur comme seul paramêtre. La méthode \code{step} n'est appellée qu'une fois par époque.
  lr_decay:
    original: |-
      multiplies the initial learning rate by \code{lr_decay} every
      \code{step_size} epochs. Unused if \code{lr_scheduler} is a \code{torch::lr_scheduler}
      or \code{NULL}.
    translation: |-
      multiplie le taux d'apprentissage initial par \code{lr_decay} à chaque \code{step_size}
      époque. Inopérant lorque \code{lr_scheduler} est un \code{torch::lr_scheduler}
      ou est \code{NULL}.
  step_size:
    original: |-
      the learning rate scheduler step size. Unused if
      \code{lr_scheduler} is a \code{torch::lr_scheduler} or \code{NULL}.
    translation: |-
      nombre de pas de calculs pour décrementer le taux d'apprentissage. Inopérant lorque 
      \code{lr_scheduler} est un \code{torch::lr_scheduler} ou est \code{NULL}.
  checkpoint_epochs:
    original: |-
      checkpoint model weights and architecture every
      \code{checkpoint_epochs}. (default is 10). This may cause large memory usage.
      Use \code{0} to disable checkpoints.
    translation: |-
      crée un checkpoint des poids et de l'architecture du modèle tous les \code{checkpoint_epochs}
      époques. la valeur 0 desactive l'enregistrement de checkpoints.
  cat_emb_dim:
    original: |-
      Size of the embedding of categorical features. If int, all categorical
      features will have same embedding size, if list of int, every corresponding feature will have
      specific embedding size.
    translation: |-
      Dimension d'encodage pour les variables catégorielles. Si c'est un entier, il est utilisé 
      pour toutes les variables catégorielles. Si c'est une liste d'entiers, chaque variable catégorielle
      sera encodée à la dimension correspondante.
  num_independent:
    original: |-
      Number of independent Gated Linear Units layers at each step of the encoder.
      Usual values range from 1 to 5.
    translation: |-
      Nombre de Gated-Linear Unit indépendantes de l'encodeur à chaque step de l'architecture.
  num_shared:
    original: |-
      Number of shared Gated Linear Units at each step of the encoder. Usual values
      at each step of the decoder. range from 1 to 5
    translation: |-
      Nombre de Gated-Linear Unit partagées de l'encodeur à chaque step de l'architecture.
      Typiquement entre 1 à 5.
  num_independent_decoder:
    original: |-
      For pretraining, number of independent Gated Linear Units layers
      Usual values range from 1 to 5.
    translation: |-
      Nombre de Gated-Linear Unit indépendantes du décodeur à chaque step de l'architecture.
      Typiquement entre 1 à 5. (Pré-entrainement seulement)
  num_shared_decoder:
    original: |-
      For pretraining, number of shared Gated Linear Units at each step of the
      decoder. Usual values range from 1 to 5.
    translation: |-
      Nombre de Gated-Linear Unit partagées du décodeur à chaque step de l'architecture.
      Typiquement entre 1 à 5. (Pré-entraînement seulement)
  momentum:
    original: |-
      Momentum for batch normalization, typically ranges from 0.01
      to 0.4 (default=0.02)
    translation: |-
      Momentum utilisé pour la normalisation du batch. Typiquement entre 0.01 et 0.4. 0.02 par défaut.
  pretraining_ratio:
    original: |-
      Ratio of features to mask for reconstruction during
      pretraining.  Ranges from 0 to 1 (default=0.5)
    translation: |-
      Ratio de variables à reconstruire pendant le pré-entraînement. À prendre entre
      0 et 1. 0.5 par défaut.
  verbose:
    original: |-
      (logical) Whether to print progress and loss values during
      training.
    translation: |-
      (logique) Doit-on afficher la progression et les valeurs de la fonction de coût
      pendant l'apprentissage
  device:
    original: |-
      the device to use for training. "cpu" or "cuda". The default ("auto")
      uses  to "cuda" if it's available, otherwise uses "cpu".
    translation: |-
      la carte utilisé pour l'entraînement, entre "cpu" pour le processeur et "cuda"
      pour la carte graphique. Par défaut "auto" utilise "cuda" si c'est disponible, sinon "cpu".
  importance_sample_size:
    original: |-
      sample of the dataset to compute importance metrics.
      If the dataset is larger than 1e5 obs we will use a sample of size 1e5 and
      display a warning.
    translation: |-
      taille du jeu de donnée pour le calcul de l'importance des variables. Si le jeu de données
      est plus grand que 1e5 observations, il sera échantillonné à 1e5 observations avec un
      message de Warning.
  early_stopping_monitor:
    original: Metric to monitor for early_stopping. One of "valid_loss", "train_loss"
      or "auto" (defaults to "auto").
    translation: |-
      Métrique à monitorer pour l'early-stopping. Parmi "valid-loss", "train-loss", et "auto"
      pour respectivement la valeur de la fonction de coût sur le jeu de données 
      de validation, d'entraînement, et le premier des deux disponibles.
  early_stopping_tolerance:
    original: |-
      Minimum relative improvement to reset the patience counter.
      0.01 for 1\% tolerance (default 0)
    translation: |-
      Valeur minimum d'amélioration entre deux époques pour remettre à zero le compteur
      de patience.
  early_stopping_patience:
    original: Number of epochs without improving until stopping training. (default=5)
    translation: |-
      Nombre d'époques sans amélioration avant d'arrêter l'entraînement. (5 par défaut)
  num_workers:
    original: |-
      (int, optional): how many subprocesses to use for data
      loading. 0 means that the data will be loaded in the main process.
      (default: \code{0})
    translation: |-
      (entier, optionnel) Nombre de sous-process à utiliser pour le chargement des données.
      \code{0} pour n'utiliser que le process principal. (\code{0} par défaut)
  skip_importance:
    original: 'if feature importance calculation should be skipped (default: \code{FALSE})'
    translation: |-
      bypass du calcul de l'importance des variables
value:
  original: |
    A named list with all hyperparameters of the TabNet implementation.
  translation: |-
    Une liste nomée de tous les hyper-paramêtres de l'implémentation de TabNet.
description:
  original: |
    Configuration for TabNet models
  translation: |-
    Configuration du modèle TabNet
untranslatable:
- alias
- name
- keyword
- concept
- usage
